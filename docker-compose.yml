version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka:
    image: confluentinc/cp-kafka:7.0.1
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9092"]
      interval: 10s
      timeout: 5s
      retries: 5

  producer:
    build: ./producer
    environment:
      - PYTHONPATH=/app
    container_name: producer
    command: python producer.py
    depends_on:
      kafka:
        condition: service_healthy

  flask-app:
    build: ./flask-app
    container_name: flask-app
    ports:
      - "5000:5000"
    depends_on:
      kafka:
        condition: service_healthy
      redis:
        condition: service_healthy

  redis:
    image: "redis:alpine"
    container_name: redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis-commander:
    image: rediscommander/redis-commander:latest
    container_name: redis-commander
    environment:
      - REDIS_HOSTS=redis
    ports:
      - "8081:8081"
    depends_on:
      - redis

  archiver:
    build: ./archiver
    container_name: archiver
    command: python archiver.py
    depends_on:
      redis:
        condition: service_healthy
      namenode:
        condition: service_started
      datanode:
        condition: service_started
    environment:
      REDIS_HOST: redis
      HDFS_NAMENODE_HOST: namenode
    networks:
      - hadoop-network
      - default

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - "9870:9870"
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
      - ./hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env
    networks:
      - hadoop-network

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
      - ./hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    networks:
      - hadoop-network

  hive-metastore-db:
    image: postgres:11
    container_name: hive-metastore-db
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: metastore
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive -d metastore"]
      interval: 5s
      timeout: 5s
      retries: 5

  hive-metastore:
    image: apache/hive:3.1.3
    container_name: hive-metastore
    command: /opt/hive/bin/hive --service metastore
    mem_limit: 2g
    environment:
      HIVE_METASTORE_DB_HOSTNAME: hive-metastore-db
      HIVE_METASTORE_DB_PORT: 5432
      HIVE_METASTORE_DB_USERNAME: hive
      HIVE_METASTORE_DB_PASSWORD: hive
      HIVE_CONFIG_DIR: /opt/hive/conf
      HADOOP_HOME: /opt/hadoop
      HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
    volumes:
      - ./hive-site.xml:/opt/hive/conf/hive-site.xml
    depends_on:
      hive-metastore-db:
        condition: service_healthy
      namenode:
        condition: service_started
      datanode:
        condition: service_started
    networks:
      - hadoop-network
      - default

  hive-server:
    image: apache/hive:3.1.3
    container_name: hive-server
    command: /opt/hive/bin/hive --service hiveserver2
    environment:
      HIVE_METASTORE_DB_HOSTNAME: hive-metastore-db
      HIVE_METASTORE_DB_PORT: 5432
      HIVE_METASTORE_DB_USERNAME: hive
      HIVE_METASTORE_DB_PASSWORD: hive
      HIVE_CONFIG_DIR: /opt/hive/conf
      HADOOP_HOME: /opt/hadoop
      HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
    ports:
      - "10000:10000" # HiveServer2 Thrift port
      - "10002:10002" # HiveServer2 HTTP port
    volumes:
      - ./hive-site.xml:/opt/hive/conf/hive-site.xml
    depends_on:
      hive-metastore:
        condition: service_started
    networks:
      - hadoop-network
      - default

  spark-master:
    build: ./spark
    container_name: spark-master
    command: bin/spark-class org.apache.spark.deploy.master.Master &
    ports:
      - "8088:8080" # Spark Master Web UI
      - "7077:7077" # Spark Master internal communication
    volumes:
      - ./core-site.xml:/opt/spark/conf/core-site.xml
    environment:
      SPARK_MODE: master
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: "no"
      SPARK_SSL_ENABLED: "no"
      SPARK_MASTER_WEBUI_PORT: 8080
      SPARK_MASTER_PORT: 7077
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_OPTS: "-Dspark.deploy.defaultCores=1 -Dspark.deploy.defaultMemory=1g"
      HADOOP_CONF_DIR: /opt/spark/conf
    depends_on:
      namenode:
        condition: service_started
      datanode:
        condition: service_started
    networks:
      - hadoop-network
      - default

  spark-worker:
    build: ./spark
    container_name: spark-worker
    depends_on:
      spark-master:
        condition: service_started
    volumes:
      - ./core-site.xml:/opt/spark/conf/core-site.xml
    environment:
      SPARK_MODE: worker
      SPARK_MASTER: spark://spark-master:7077
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 1g
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
      SPARK_RPC_ENCRYPTION_ENABLED: "no"
      SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED: "no"
      SPARK_SSL_ENABLED: "no"
      HADOOP_CONF_DIR: /opt/spark/conf
    networks:
      - hadoop-network
      - default

volumes:
  hadoop_namenode:
  hadoop_datanode:

networks:
  hadoop-network:
    driver: bridge
